{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Learning to Rank in PyTorch \u00b6 Introduction \u00b6 This open-source project, referred to as PTRanking (Learning to Rank in PyTorch) aims to provide scalable and extendable implementations of typical learning-to-rank methods based on PyTorch. On one hand, this project enables a uniform comparison over several benchmark datasets leading to an in-depth understanding of previous learning-to-rank methods. On the other hand, this project makes it easy to develop and incorporate newly proposed models, so as to expand the territory of techniques on learning-to-rank. Key Features : A number of representative learning-to-rank models, including not only the traditional optimization framework via empirical risk minimization but also the adversarial optimization framework Supports widely used benchmark datasets. Meanwhile, random masking of the ground-truth labels with a specified ratio is also supported Supports different metrics, such as Precision, MAP, nDCG and nERR Highly configurable functionalities for fine-tuning hyper-parameters, e.g., grid-search over hyper-parameters of a specific model Provides easy-to-use APIs for developing a new learning-to-rank model How-to-Start and Learning more \u00b6 Demo Scripts \u00b6 To get a taste of learning-to-rank models without writing any code, you could try the following script. You just need to specify the model name, the dataset id, as well as the directories for input and output. Jupyter Notebook example on RankNet & LambdaRank To get familiar with the process of data loading, you could try the following script, namely, get the statistics of a dataset. Jupyter Notebook example on getting dataset statistics Develop A New Model \u00b6 PT-Ranking offers deep neural networks as the basis to construct a scoring function based on PyTorch and can thus fully leverage the advantages of PyTorch. NeuralRanker is a class that represents a general learning-to-rank model. A key component of NeuralRanker is the neural scoring function. The configurable hyper-parameters include activation function, number of layers, number of neurons per layer, etc. All specific learning-to-rank models inherit NeuralRanker and mainly differ in the way of computing the training loss. The following figure shows the main step in developing a new learning-to-rank model based on Empirical Risk Minimization, where batch_preds and batch_stds correspond to outputs of the scoring function and ground-truth lables, respectively. We can observe that the main work is to define the surrogate loss function. Parameter Setting \u00b6 An easy-to-use parameter setting is necessary for any ML library. PT-Ranking offers a self-contained strategy. In other words, we appeals to particularly designed class objects for setting. For example, DataSetting for data loading, EvalSetting for evaluation setting and ModelParameter for a model's parameter setting. When incorporating a newly developed model (say ModelA), it is commonly required to develop the subclass ModelAParameter by inheriting ModelParameter and customizing the functions, such as to_para_string(), default_para_dict() and grid_search(). Please refer to LambdaRankParameter as an example. Thanks to this strategy, on one hand, we can initialize the settings for data-loading, evaluation, and models in a simple way. On the other hand, the parameter setting of a model is self-contained, and easy to customize. To fully leverage PT-Ranking, one needs to be familiar with PyTorch . For detailed introduction on learning-to-rank, please refer to the book: Learning to Rank for Information Retrieval . Source Codes \u00b6 Please refer to the GitHub Repository for PTRanking's implementation details. Call for Contribution \u00b6 We are adding more learning-to-rank models all the time. Please submit an issue if there is something you want to have implemented and included. Meanwhile, anyone who are interested in any kinds of contributions and/or collaborations are warmly welcomed. Relevant Resources \u00b6 Name Language Deep Learning PTRanking Python PyTorch TF-Ranking Python TensorFlow MatchZoo Python Keras / PyTorch Shoelace Python Chainer LEROT Python x Rank Lib Java x Propensity SVM^Rank C x QuickRank C++ x","title":"Home"},{"location":"#learning-to-rank-in-pytorch","text":"","title":"Learning to Rank in PyTorch"},{"location":"#introduction","text":"This open-source project, referred to as PTRanking (Learning to Rank in PyTorch) aims to provide scalable and extendable implementations of typical learning-to-rank methods based on PyTorch. On one hand, this project enables a uniform comparison over several benchmark datasets leading to an in-depth understanding of previous learning-to-rank methods. On the other hand, this project makes it easy to develop and incorporate newly proposed models, so as to expand the territory of techniques on learning-to-rank. Key Features : A number of representative learning-to-rank models, including not only the traditional optimization framework via empirical risk minimization but also the adversarial optimization framework Supports widely used benchmark datasets. Meanwhile, random masking of the ground-truth labels with a specified ratio is also supported Supports different metrics, such as Precision, MAP, nDCG and nERR Highly configurable functionalities for fine-tuning hyper-parameters, e.g., grid-search over hyper-parameters of a specific model Provides easy-to-use APIs for developing a new learning-to-rank model","title":"Introduction"},{"location":"#how-to-start-and-learning-more","text":"","title":"How-to-Start and Learning more"},{"location":"#demo-scripts","text":"To get a taste of learning-to-rank models without writing any code, you could try the following script. You just need to specify the model name, the dataset id, as well as the directories for input and output. Jupyter Notebook example on RankNet & LambdaRank To get familiar with the process of data loading, you could try the following script, namely, get the statistics of a dataset. Jupyter Notebook example on getting dataset statistics","title":"Demo Scripts"},{"location":"#develop-a-new-model","text":"PT-Ranking offers deep neural networks as the basis to construct a scoring function based on PyTorch and can thus fully leverage the advantages of PyTorch. NeuralRanker is a class that represents a general learning-to-rank model. A key component of NeuralRanker is the neural scoring function. The configurable hyper-parameters include activation function, number of layers, number of neurons per layer, etc. All specific learning-to-rank models inherit NeuralRanker and mainly differ in the way of computing the training loss. The following figure shows the main step in developing a new learning-to-rank model based on Empirical Risk Minimization, where batch_preds and batch_stds correspond to outputs of the scoring function and ground-truth lables, respectively. We can observe that the main work is to define the surrogate loss function.","title":"Develop A New Model"},{"location":"#parameter-setting","text":"An easy-to-use parameter setting is necessary for any ML library. PT-Ranking offers a self-contained strategy. In other words, we appeals to particularly designed class objects for setting. For example, DataSetting for data loading, EvalSetting for evaluation setting and ModelParameter for a model's parameter setting. When incorporating a newly developed model (say ModelA), it is commonly required to develop the subclass ModelAParameter by inheriting ModelParameter and customizing the functions, such as to_para_string(), default_para_dict() and grid_search(). Please refer to LambdaRankParameter as an example. Thanks to this strategy, on one hand, we can initialize the settings for data-loading, evaluation, and models in a simple way. On the other hand, the parameter setting of a model is self-contained, and easy to customize. To fully leverage PT-Ranking, one needs to be familiar with PyTorch . For detailed introduction on learning-to-rank, please refer to the book: Learning to Rank for Information Retrieval .","title":"Parameter Setting"},{"location":"#source-codes","text":"Please refer to the GitHub Repository for PTRanking's implementation details.","title":"Source Codes"},{"location":"#call-for-contribution","text":"We are adding more learning-to-rank models all the time. Please submit an issue if there is something you want to have implemented and included. Meanwhile, anyone who are interested in any kinds of contributions and/or collaborations are warmly welcomed.","title":"Call for Contribution"},{"location":"#relevant-resources","text":"Name Language Deep Learning PTRanking Python PyTorch TF-Ranking Python TensorFlow MatchZoo Python Keras / PyTorch Shoelace Python Chainer LEROT Python x Rank Lib Java x Propensity SVM^Rank C x QuickRank C++ x","title":"Relevant Resources"},{"location":"data/","text":"Supported Datasets and Formats \u00b6 Popular Benchmark Datasets \u00b6 -- LETOR4.0 (MQ2007 | MQ2008 | MQ2007-semi | MQ2008-semi | MQ2007-list | MQ2008-list ) -- MSLR-WEB (MSLR-WEB10K | MSLR-WEB30K) -- Yahoo! LETOR (including Set1 | Set2) -- Istella (Istella-S | Istella | Istella-X) These above datasets can be directly used once downloaded. Please note that: Semi-supervised datasets (MQ2007-semi | MQ2008-semi) have the same format as that for supervised ranking setting. The only difference is that the semi-supervised datasets in this setting contain both judged and undged query-document pairs ( in training set but not in validation and testing set )(The relevance label \u201c-1\u201d indicates the query-document pair is not judged) while the datasets for supervised ranking contain only judged query-document pair. According to Introducing LETOR 4.0 Datasets , queryLevelNorm version refers to that: conduct query level normalization in the way of using MIN. This data can be directly used for learning. They further provide 5-fold partitions of this version for cross fold validation. Thus there is no need to perform query_level_scale again for {MQ2007_super | MQ2008_super | MQ2007_semi | MQ2008_semi}. But for {MSLRWEB10K | MSLRWEB30K}, the query-level normalization is not conducted yet . For Yahoo! LETOR, the query-level normalization is already done. For Istella! LETOR, the query-level normalization is not conducted yet . We note that ISTELLA contains extremely large features, e.g., 1.79769313486e+308, we replace features of this kind with a constant 1000000. LibSVM formats \u00b6 PT-Ranking currently supports to ingest data with the LibSVM formats LETOR datasets in LibSVM format \\<ground-truth label int> qid\\: : ... : For example: 4 qid:105 2:0.4 8:0.7 50:0.5 1 qid:105 5:0.5 30:0.7 32:0.4 48:0.53 0 qid:210 4:0.9 38:0.01 39:0.5 45:0.7 1 qid:210 1:0.2 8:0.9 31:0.93 40:0.6 The above sample dataset includes two queries, the query \"105\" has 2 documents, the corresponding ground-truth labels are 4 and 1, respectively. Converting LETOR datasets into LibSVM format with a corresponding group file This functionality is required when using the implementation of LambdaMART provided in LightGBM .","title":"Data"},{"location":"data/#supported-datasets-and-formats","text":"","title":"Supported Datasets and Formats"},{"location":"data/#popular-benchmark-datasets","text":"-- LETOR4.0 (MQ2007 | MQ2008 | MQ2007-semi | MQ2008-semi | MQ2007-list | MQ2008-list ) -- MSLR-WEB (MSLR-WEB10K | MSLR-WEB30K) -- Yahoo! LETOR (including Set1 | Set2) -- Istella (Istella-S | Istella | Istella-X) These above datasets can be directly used once downloaded. Please note that: Semi-supervised datasets (MQ2007-semi | MQ2008-semi) have the same format as that for supervised ranking setting. The only difference is that the semi-supervised datasets in this setting contain both judged and undged query-document pairs ( in training set but not in validation and testing set )(The relevance label \u201c-1\u201d indicates the query-document pair is not judged) while the datasets for supervised ranking contain only judged query-document pair. According to Introducing LETOR 4.0 Datasets , queryLevelNorm version refers to that: conduct query level normalization in the way of using MIN. This data can be directly used for learning. They further provide 5-fold partitions of this version for cross fold validation. Thus there is no need to perform query_level_scale again for {MQ2007_super | MQ2008_super | MQ2007_semi | MQ2008_semi}. But for {MSLRWEB10K | MSLRWEB30K}, the query-level normalization is not conducted yet . For Yahoo! LETOR, the query-level normalization is already done. For Istella! LETOR, the query-level normalization is not conducted yet . We note that ISTELLA contains extremely large features, e.g., 1.79769313486e+308, we replace features of this kind with a constant 1000000.","title":"Popular Benchmark Datasets"},{"location":"data/#libsvm-formats","text":"PT-Ranking currently supports to ingest data with the LibSVM formats LETOR datasets in LibSVM format \\<ground-truth label int> qid\\: : ... : For example: 4 qid:105 2:0.4 8:0.7 50:0.5 1 qid:105 5:0.5 30:0.7 32:0.4 48:0.53 0 qid:210 4:0.9 38:0.01 39:0.5 45:0.7 1 qid:210 1:0.2 8:0.9 31:0.93 40:0.6 The above sample dataset includes two queries, the query \"105\" has 2 documents, the corresponding ground-truth labels are 4 and 1, respectively. Converting LETOR datasets into LibSVM format with a corresponding group file This functionality is required when using the implementation of LambdaMART provided in LightGBM .","title":"LibSVM formats"}]}